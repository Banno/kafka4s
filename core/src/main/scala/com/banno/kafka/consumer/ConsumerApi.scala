/*
 * Copyright 2019 Jack Henry & Associates, Inc.Â®
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.banno.kafka.consumer

import cats.*
import cats.arrow.*
import cats.effect.*
import cats.syntax.all.*
import com.banno.kafka.*
import fs2.Stream
import io.confluent.kafka.serializers.KafkaAvroDeserializer
import java.util.concurrent.{Executors, ThreadFactory}
import java.util.regex.Pattern
import org.apache.avro.generic.GenericRecord
import org.apache.kafka.clients.consumer.*
import org.apache.kafka.common.*
import org.apache.kafka.common.serialization.Deserializer
import scala.concurrent.duration.*
import scala.concurrent.{ExecutionContext, ExecutionContextExecutorService}
import scala.jdk.CollectionConverters.*

trait ConsumerApi[F[_], K, V] {
  def assign(partitions: Iterable[TopicPartition]): F[Unit]
  def assignment: F[Set[TopicPartition]]

  def close: F[Unit]
  def close(timeout: FiniteDuration): F[Unit]
  def commitAsync: F[Unit]
  def commitAsync(
      offsets: Map[TopicPartition, OffsetAndMetadata],
      callback: OffsetCommitCallback,
  ): F[Unit]
  def commitAsync(callback: OffsetCommitCallback): F[Unit]
  def commitSync: F[Unit]
  def commitSync(offsets: Map[TopicPartition, OffsetAndMetadata]): F[Unit]

  def listTopics: F[Map[String, Seq[PartitionInfo]]]
  def listTopics(timeout: FiniteDuration): F[Map[String, Seq[PartitionInfo]]]
  def metrics: F[Map[MetricName, Metric]]

  def pause(partitions: Iterable[TopicPartition]): F[Unit]
  def paused: F[Set[TopicPartition]]
  def poll(timeout: FiniteDuration): F[ConsumerRecords[K, V]]
  def position(partition: TopicPartition): F[Long]
  def resume(partitions: Iterable[TopicPartition]): F[Unit]
  def seek(partition: TopicPartition, offset: Long): F[Unit]
  def seekToBeginning(partitions: Iterable[TopicPartition]): F[Unit]
  def seekToEnd(partitions: Iterable[TopicPartition]): F[Unit]
  def subscribe(topics: Iterable[String]): F[Unit]
  def subscribe(
      topics: Iterable[String],
      callback: ConsumerRebalanceListener,
  ): F[Unit]
  def subscribe(pattern: Pattern): F[Unit]
  def subscribe(pattern: Pattern, callback: ConsumerRebalanceListener): F[Unit]
  def subscription: F[Set[String]]
  def unsubscribe: F[Unit]

  def partitionQueries: PartitionQueries[F]

  final def mapK[G[_]](f: F ~> G): ConsumerApi[G, K, V] = {
    val self = this
    new ConsumerApi[G, K, V] {
      override def assign(partitions: Iterable[TopicPartition]): G[Unit] =
        f(self.assign(partitions))
      override def assignment: G[Set[TopicPartition]] =
        f(self.assignment)

      override def close: G[Unit] =
        f(self.close)
      override def close(timeout: FiniteDuration): G[Unit] =
        f(self.close(timeout))

      override def commitAsync: G[Unit] =
        f(self.commitAsync)
      override def commitAsync(
          offsets: Map[TopicPartition, OffsetAndMetadata],
          callback: OffsetCommitCallback,
      ): G[Unit] =
        f(self.commitAsync(offsets, callback))
      override def commitAsync(callback: OffsetCommitCallback): G[Unit] =
        f(self.commitAsync(callback))
      override def commitSync: G[Unit] =
        f(self.commitSync)
      override def commitSync(
          offsets: Map[TopicPartition, OffsetAndMetadata]
      ): G[Unit] =
        f(self.commitSync(offsets))

      override def listTopics: G[Map[String, Seq[PartitionInfo]]] =
        f(self.listTopics)
      override def listTopics(
          timeout: FiniteDuration
      ): G[Map[String, Seq[PartitionInfo]]] =
        f(self.listTopics(timeout))
      override def metrics: G[Map[MetricName, Metric]] =
        f(self.metrics)

      override def pause(partitions: Iterable[TopicPartition]): G[Unit] =
        f(self.pause(partitions))
      override def paused: G[Set[TopicPartition]] =
        f(self.paused)
      override def poll(timeout: FiniteDuration): G[ConsumerRecords[K, V]] =
        f(self.poll(timeout))
      override def position(partition: TopicPartition): G[Long] =
        f(self.position(partition))
      override def resume(partitions: Iterable[TopicPartition]): G[Unit] =
        f(self.resume(partitions))
      override def seek(partition: TopicPartition, offset: Long): G[Unit] =
        f(self.seek(partition, offset))
      override def seekToBeginning(
          partitions: Iterable[TopicPartition]
      ): G[Unit] =
        f(self.seekToBeginning(partitions))
      override def seekToEnd(partitions: Iterable[TopicPartition]): G[Unit] =
        f(self.seekToEnd(partitions))
      override def subscribe(topics: Iterable[String]): G[Unit] =
        f(self.subscribe(topics))
      override def subscribe(
          topics: Iterable[String],
          callback: ConsumerRebalanceListener,
      ): G[Unit] =
        f(self.subscribe(topics, callback))
      override def subscribe(pattern: Pattern): G[Unit] =
        f(self.subscribe(pattern))
      override def subscribe(
          pattern: Pattern,
          callback: ConsumerRebalanceListener,
      ): G[Unit] =
        f(self.subscribe(pattern, callback))
      override def subscription: G[Set[String]] =
        f(self.subscription)
      override def unsubscribe: G[Unit] =
        f(self.unsubscribe)

      override def partitionQueries: PartitionQueries[G] =
        self.partitionQueries.mapK(f)
    }
  }

  final def biSemiflatMap[C, D](
      f: K => F[C],
      g: V => F[D],
  )(implicit F: Monad[F]): ConsumerApi[F, C, D] = {
    val fab = this
    new ConsumerApi[F, C, D] {
      override def assign(partitions: Iterable[TopicPartition]): F[Unit] =
        fab.assign(partitions)
      override def assignment: F[Set[TopicPartition]] =
        fab.assignment

      override def close: F[Unit] =
        fab.close
      override def close(timeout: FiniteDuration): F[Unit] =
        fab.close(timeout)

      override def commitAsync: F[Unit] =
        fab.commitAsync
      override def commitAsync(
          offsets: Map[TopicPartition, OffsetAndMetadata],
          callback: OffsetCommitCallback,
      ): F[Unit] =
        fab.commitAsync(offsets, callback)
      override def commitAsync(callback: OffsetCommitCallback): F[Unit] =
        fab.commitAsync(callback)
      override def commitSync: F[Unit] =
        fab.commitSync
      override def commitSync(
          offsets: Map[TopicPartition, OffsetAndMetadata]
      ): F[Unit] =
        fab.commitSync(offsets)

      override def listTopics: F[Map[String, Seq[PartitionInfo]]] =
        fab.listTopics
      override def listTopics(
          timeout: FiniteDuration
      ): F[Map[String, Seq[PartitionInfo]]] =
        fab.listTopics(timeout)
      override def metrics: F[Map[MetricName, Metric]] =
        fab.metrics

      override def pause(partitions: Iterable[TopicPartition]): F[Unit] =
        fab.pause(partitions)
      override def paused: F[Set[TopicPartition]] =
        fab.paused
      override def poll(timeout: FiniteDuration): F[ConsumerRecords[C, D]] =
        fab.poll(timeout).flatMap(_.bitraverse(f, g))
      override def position(partition: TopicPartition): F[Long] =
        fab.position(partition)
      override def resume(partitions: Iterable[TopicPartition]): F[Unit] =
        fab.resume(partitions)
      override def seek(partition: TopicPartition, offset: Long): F[Unit] =
        fab.seek(partition, offset)
      override def seekToBeginning(
          partitions: Iterable[TopicPartition]
      ): F[Unit] =
        fab.seekToBeginning(partitions)
      override def seekToEnd(
          partitions: Iterable[TopicPartition]
      ): F[Unit] =
        fab.seekToEnd(partitions)
      override def subscribe(topics: Iterable[String]): F[Unit] =
        fab.subscribe(topics)
      override def subscribe(
          topics: Iterable[String],
          callback: ConsumerRebalanceListener,
      ): F[Unit] =
        fab.subscribe(topics, callback)
      override def subscribe(pattern: Pattern): F[Unit] =
        fab.subscribe(pattern)
      override def subscribe(
          pattern: Pattern,
          callback: ConsumerRebalanceListener,
      ): F[Unit] =
        fab.subscribe(pattern, callback)
      override def subscription: F[Set[String]] =
        fab.subscription
      override def unsubscribe: F[Unit] =
        fab.unsubscribe

      override def partitionQueries: PartitionQueries[F] =
        fab.partitionQueries
    }
  }
}

/** Since the underlying Java KafkaConsumer is (as documented) *not*
  * thread-safe, this can be used to shift all operations on to an execution
  * context that makes this safe (e.g. a single-thread pool). Currently, this is
  * the safest way to use ConsumerImpl, and is used in all ConsumerApi-creating
  * operations below, that are used in practice.
  */
object ShiftingConsumer {
  def apply[F[_]: Async, K, V](
      c: ConsumerApi[F, K, V],
      blockingContext: ExecutionContext,
  ): ConsumerApi[F, K, V] =
    c.mapK(
      FunctionK.liftFunction(
        Async[F].evalOn(_, blockingContext)
      )
    )
}

object ConsumerApi {
  implicit def bifunctor[F[_]: Functor]: Bifunctor[ConsumerApi[F, *, *]] =
    new Bifunctor[ConsumerApi[F, *, *]] {
      override def bimap[A, B, C, D](
          fab: ConsumerApi[F, A, B]
      )(
          f: A => C,
          g: B => D,
      ): ConsumerApi[F, C, D] =
        new ConsumerApi[F, C, D] {
          override def assign(partitions: Iterable[TopicPartition]): F[Unit] =
            fab.assign(partitions)
          override def assignment: F[Set[TopicPartition]] =
            fab.assignment

          override def close: F[Unit] =
            fab.close
          override def close(timeout: FiniteDuration): F[Unit] =
            fab.close(timeout)

          override def commitAsync: F[Unit] =
            fab.commitAsync
          override def commitAsync(
              offsets: Map[TopicPartition, OffsetAndMetadata],
              callback: OffsetCommitCallback,
          ): F[Unit] =
            fab.commitAsync(offsets, callback)
          override def commitAsync(callback: OffsetCommitCallback): F[Unit] =
            fab.commitAsync(callback)
          override def commitSync: F[Unit] =
            fab.commitSync
          override def commitSync(
              offsets: Map[TopicPartition, OffsetAndMetadata]
          ): F[Unit] =
            fab.commitSync(offsets)

          override def listTopics: F[Map[String, Seq[PartitionInfo]]] =
            fab.listTopics
          override def listTopics(
              timeout: FiniteDuration
          ): F[Map[String, Seq[PartitionInfo]]] =
            fab.listTopics(timeout)
          override def metrics: F[Map[MetricName, Metric]] =
            fab.metrics

          override def pause(partitions: Iterable[TopicPartition]): F[Unit] =
            fab.pause(partitions)
          override def paused: F[Set[TopicPartition]] =
            fab.paused
          override def poll(timeout: FiniteDuration): F[ConsumerRecords[C, D]] =
            fab.poll(timeout).map(_.bimap(f, g))
          override def position(partition: TopicPartition): F[Long] =
            fab.position(partition)
          override def resume(partitions: Iterable[TopicPartition]): F[Unit] =
            fab.resume(partitions)
          override def seek(partition: TopicPartition, offset: Long): F[Unit] =
            fab.seek(partition, offset)
          override def seekToBeginning(
              partitions: Iterable[TopicPartition]
          ): F[Unit] =
            fab.seekToBeginning(partitions)
          override def seekToEnd(
              partitions: Iterable[TopicPartition]
          ): F[Unit] =
            fab.seekToEnd(partitions)
          override def subscribe(topics: Iterable[String]): F[Unit] =
            fab.subscribe(topics)
          override def subscribe(
              topics: Iterable[String],
              callback: ConsumerRebalanceListener,
          ): F[Unit] =
            fab.subscribe(topics, callback)
          override def subscribe(pattern: Pattern): F[Unit] =
            fab.subscribe(pattern)
          override def subscribe(
              pattern: Pattern,
              callback: ConsumerRebalanceListener,
          ): F[Unit] =
            fab.subscribe(pattern, callback)
          override def subscription: F[Set[String]] =
            fab.subscription
          override def unsubscribe: F[Unit] =
            fab.unsubscribe

          override def partitionQueries: PartitionQueries[F] =
            fab.partitionQueries
        }
    }

  private[this] def createKafkaConsumer[F[_]: Sync, K, V](
      configs: (String, AnyRef)*
  ): F[KafkaConsumer[K, V]] =
    Sync[F].delay(new KafkaConsumer[K, V](configs.toMap.asJava))

  private[this] def createKafkaConsumer[F[_]: Sync, K, V](
      keyDeserializer: Deserializer[K],
      valueDeserializer: Deserializer[V],
      configs: (String, AnyRef)*
  ): F[KafkaConsumer[K, V]] =
    Sync[F].delay(
      new KafkaConsumer[K, V](
        configs.toMap.asJava,
        keyDeserializer,
        valueDeserializer,
      )
    )

  object BlockingContext {

    def resource[F[_]: Sync]: Resource[F, ExecutionContextExecutorService] =
      Resource.make(
        Sync[F].delay(
          ExecutionContext.fromExecutorService(
            Executors.newSingleThreadExecutor(new ThreadFactory {
              val factory = Executors.defaultThreadFactory()
              def newThread(r: Runnable): Thread = {
                val t = factory.newThread(r)
                t.setName("kafka4s-consumer")
                t
              }
            })
          )
        )
      )(a => Sync[F].delay(a.shutdown()))
  }

  def resource[F[_]: Async, K, V](
      keyDeserializer: Deserializer[K],
      valueDeserializer: Deserializer[V],
      configs: (String, AnyRef)*
  ): Resource[F, ConsumerApi[F, K, V]] =
    BlockingContext.resource.flatMap(e =>
      Resource.make(
        createKafkaConsumer[F, K, V](
          keyDeserializer,
          valueDeserializer,
          configs: _*
        )
          .map(c => ShiftingConsumer(ConsumerImpl(c), e))
      )(_.close)
    )

  def resource[F[_]: Async, K: Deserializer, V: Deserializer](
      configs: (String, AnyRef)*
  ): Resource[F, ConsumerApi[F, K, V]] =
    resource[F, K, V](
      implicitly[Deserializer[K]],
      implicitly[Deserializer[V]],
      configs: _*
    )

  object ByteArray {
    def resource[F[_]: Async](
        configs: (String, AnyRef)*
    ): Resource[F, ConsumerApi[F, Array[Byte], Array[Byte]]] =
      ConsumerApi.resource[F, Array[Byte], Array[Byte]](configs: _*)
  }

  object Avro {

    def resource[F[_]: Async, K, V](
        configs: (String, AnyRef)*
    ): Resource[F, ConsumerApi[F, K, V]] =
      BlockingContext.resource.flatMap(e =>
        Resource.make(
          createKafkaConsumer[F, K, V](
            (
              configs.toMap +
              KeyDeserializerClass(classOf[KafkaAvroDeserializer]) +
              ValueDeserializerClass(classOf[KafkaAvroDeserializer])
            ).toSeq: _*
          ).map(c => ShiftingConsumer(ConsumerImpl(c), e))
        )(_.close)
      )

    object Generic {

      def resource[F[_]: Async](
          configs: (String, AnyRef)*
      ): Resource[F, ConsumerApi[F, GenericRecord, GenericRecord]] =
        ConsumerApi.Avro.resource[F, GenericRecord, GenericRecord](configs: _*)

      def stream[F[_]: Async](
          configs: (String, AnyRef)*
      ): Stream[F, ConsumerApi[F, GenericRecord, GenericRecord]] =
        Stream.resource(resource[F](configs: _*))
    }

    object Specific {

      def resource[F[_]: Async, K, V](
          configs: (String, AnyRef)*
      ): Resource[F, ConsumerApi[F, K, V]] =
        ConsumerApi.Avro.resource[F, K, V](
          (configs.toMap + SpecificAvroReader(s = true)).toSeq: _*
        )
    }
  }

  object NonShifting {

    def resource[F[_]: Sync, K, V](
        keyDeserializer: Deserializer[K],
        valueDeserializer: Deserializer[V],
        configs: (String, AnyRef)*
    ): Resource[F, ConsumerApi[F, K, V]] =
      Resource.make(
        createKafkaConsumer[F, K, V](
          keyDeserializer,
          valueDeserializer,
          configs: _*
        )
          .map(ConsumerImpl.create(_))
      )(_.close)

    def resource[F[_]: Sync, K: Deserializer, V: Deserializer](
        configs: (String, AnyRef)*
    ): Resource[F, ConsumerApi[F, K, V]] =
      resource[F, K, V](
        implicitly[Deserializer[K]],
        implicitly[Deserializer[V]],
        configs: _*
      )
  }
}
